# Improving Ontology Generation in LLMs through Continual Pretraining and Fast Quality Evaluation

## Overview

This repository provides a complete pipeline to:
- **Generate ontology fragments using an LLM** (default: Llama 3.2-1B)
- **Calculate detailed metrics** over both real ontology datasets and LLM-generated outputs
- **Compute the Ontology Reference Index (ORI)**, which combines multiple normalized metrics to rank and compare ontologies

---

## üì¶ Repository Structure

- **`code/` folder** ‚Üí Main working folder, containing:
    - **`data/`** ‚Üí Three sample ontologies for testing
    - **`outputs/`** ‚Üí Results generated from the example ontologies and LLM outputs
    - **`scripts/`** ‚Üí Core scripts:
        - `calculate_metrics` ‚Üí Includes `metrics.py`, `dataset_metrics.py`, `llm_metrics.py`, `ori.py` for metric calculations and ORI computation
        - `llm_output_generation` ‚Üí Runs the LLM to generate ontology fragments from prompts

- **`/results` folder** ‚Üí Contains main outputs obtained over the full dataset (1766 ontologies from DBpedia Archivo):
    - `full_dataset_with_ORI.xlsx` ‚Üí Complete dataset with all metrics and ORI
    - `llm_with_ORI.xlsx` ‚Üí Base and trained LLM results with ORI
    - `manual_evaluation.xlsx` ‚Üí Annotated manual evaluation of triple-level errors

---

## üîç Workflow: Metrics & ORI Calculation

1Ô∏è‚É£ **Run Ontology Generation**  
Input file: `prompts.json` ‚Üí Output file: `generated_ontologies.json`  
- For each ontology fragment in `prompts.json`, the LLM generates **six** independent completions.
- Example input:
    ```json
    {
        "citation": {
            "source": "edam",
            "text": ":citation rdf:type owl:AnnotationProperty ..."
        }
    }
    ```
- Example output:
    ```json
    {
        "citation_0": {
            "source": "edam",
            "generated_text": "Generated ontology for citation..."
        }
    }
    ```

2Ô∏è‚É£ **Compute Dataset Metrics**  
Run `dataset_metrics.py` to process `.ttl` files and calculate metrics.

3Ô∏è‚É£ **Compute LLM Metrics**  
Run `llm_metrics.py` on the generated outputs to calculate average metrics.

4Ô∏è‚É£ **Calculate ORI**  
Run `ori.py` to combine dataset and LLM metrics into the **Ontology Reference Index (ORI)**, providing a comparative weighted score.

---

## üê≥ Docker Setup

- Includes a **Docker Compose** (`docker-compose.yml`) with GPU support
- Uses `docker/Dockerfile` + `requirements.txt` (Python 3.10, pandas, rdflib, transformers, torch, etc.)
- Mounts scripts, data, and outputs for easy manual runs

---

## ‚ö† Disclaimer

The three sample ontologies included in the `code/data` folder are **for demonstration only**. While you can run the full pipeline on these examples, the ORI values will **not match** the main results because the normalization step depends on the full dataset (1766 ontologies from DBpedia Archivo). The `outputs` folder inside `code/` contains example results computed only on these samples and the generated LLM outputs. These are intended for testing and illustration, not for full-scale evaluation.

---
